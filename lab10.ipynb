{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksp3-wvDwawZ"
      },
      "source": [
        "# Back Bay National Wildlife Refuge\n",
        "\n",
        "\n",
        "> Back Bay National Wildlife Refuge is located in the southeastern corner of the City of Virginia Beach. The refuge was established in 1938 to protect and provide habitat for migrating and wintering waterfowl. Diverse habitats, including beachfront, freshwater marsh, dunes, shrub-scrub and upland forest are home to hundreds of species of birds, reptiles, amphibians, mammals and fish.\n",
        "\n",
        "![BNWR](https://www.fws.gov/sites/default/files/styles/banner_image_xl/public/banner_images/2020-09/waterfowl%20%28tundras%29.jpg?h=0c8d0f81&itok=NcZlpD27)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYrofsga2Ufg"
      },
      "source": [
        "To get introduced to the park and its history, please view the following interactive story map.\n",
        "\n",
        "[BBNWR History and Introduction](https://storymaps.arcgis.com/stories/960d9db38cca4f3d8d38111119b9874f)\n",
        "\n",
        "Additionally, here is some drone footage of the park for a better look at the geography and ecology of the area.\n",
        "\n",
        "[BBNWR Drone Footage](https://www.youtube.com/watch?v=NlW330aBTCc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA0hA9m5wawa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfeXpob9wawb"
      },
      "outputs": [],
      "source": [
        "bbnwr = pd.read_csv(\"https://github.com/UM-Data-Science-101/lab-10/raw/refs/heads/main/BKB_WaterQualityData_2020084.csv\")\n",
        "bbnwr.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bbnwr.head()"
      ],
      "metadata": {
        "id": "DDcJk-ZBO-u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vgIo90Dn2fS"
      },
      "outputs": [],
      "source": [
        "bbnwr[\"Site_Id\"].replace('d', 'D', inplace = True)\n",
        "bbnwr[\"Site_Id\"].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvaafA4OlZJT"
      },
      "outputs": [],
      "source": [
        "dissox = bbnwr[\"Dissolved Oxygen (mg/L)\"].dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJKz38G7ld31"
      },
      "source": [
        "## Testing Hypotheses\n",
        "\n",
        "In the previous lab and homework, we investigated the behavior of the sample mean as our sample size grew larger. As we would expect from the Central Limit Theorem, as the sample size got bigger and bigger, the sample mean behaved more and more like a Gaussian distribution, centered at the population mean and with a standard deviation equal to the standard error.\n",
        "\n",
        "Let's apply those ideas to the original BNWR data, treating it as a random sample from the population of possible measurements that could have occurred (perhaps best thought of as a data generating process).\n",
        "\n",
        "\n",
        "Using `dissox` as the sample, compute the SEM as the ratio of the sample standard deviation to the square root of the sample size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFdj0U2tld32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R8VRXMWld32"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "stdx = dissox.std()\n",
        "n = len(dissox)\n",
        "sem = stdx / np.sqrt(n)\n",
        "\n",
        "sem\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H6Jh_qdld32"
      },
      "source": [
        "Now we have estimated the standard error of the mean. Consider the hypothesis that the true population average dissolved oxygen is 7 mg/L. If this was the truth, what range of values would include the sample mean approximately 95% of the time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmJejjZald32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTJw_YLjld32"
      },
      "source": [
        "<details>\n",
        "\n",
        "`(7 - 2 * sem, 7 + 2 * sem)`\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7sMzfdmld32"
      },
      "source": [
        "Compute the sample mean. Is it in that iterval. At the 5% $\\alpha$ (confidence) level, would you reject the null hypothesis that the population average dissolved oxygen in 7 in favor of the alternative that it is not 7?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxsqfKRIld32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlYXRBjXld32"
      },
      "source": [
        "<details>\n",
        "\n",
        "`dissox.mean()`\n",
        "\n",
        "We see the sample means falls out side this range, so we would reject the null hypothesis at the 5% level.\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgYGA9DHld32"
      },
      "source": [
        "Now do a hypothesis test of the hypothesis that the average population \"Salinity (ppt)\" is 0.8 against the alternative that it is not equal to 0.8. Use a 5% $\\alpha$ level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E7qzp01ld32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w5O-RL9ld32"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "salinity = bbnwr[\"Salinity (ppt)\"].dropna()\n",
        "salinity_sem = salinity.std() / np.sqrt(len(salinity))\n",
        "(0.8 - 2 * salinity_sem, 0.8 + 2 * salinity_sem)\n",
        "    \n",
        "salinity.mean()\n",
        "```\n",
        "\n",
        "We see that the sample mean is  outside of this range, so we reject the null hypothesis at the 5% level.\n",
        "    \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iFeQKumld33"
      },
      "source": [
        "We could also compute a \"Z test\" by subtracting the hypothesized value form the sample mean and dividing by the SEM. The result should be approximately a Gaussian distribution with mean zero and variance one if the hypothesis is true.\n",
        "\n",
        "Compute the Z test for Salinity.\n",
        "\n",
        "How many standard deviations are we away from the hypothesized population mean? If the hypothesis that the population mean was 0.8 was true what is the approximate probability of observing a sample mean that is as far or farther than the observed sample mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6ZAueA2ld33"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j1zr_4Qld33"
      },
      "source": [
        "<details>\n",
        "\n",
        "`(salinity.mean() - 0.8) / sem`\n",
        "\n",
        "1.3 standard deviations away. The emprical rule doesn't quite fit 1.2 standard deviations, but we can roughly interpolate. This is somewhere between 32% and 5%, so probably close to 20% of sample means will be 1.3 standard deviations away from the mean. A fairly common event.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXFYctfFzTyK"
      },
      "source": [
        "## From Testing to p-values\n",
        "\n",
        "Above, we performed two hypothesis tests. In the first, we tested the null hypothesis that mean `dissox` value in the population was equal to 7 mg/L against the alternative that it was not 7 mg/L. In the second test, we tested the null hypothesis the average salinity in the population was 0.8, again against the two-sided alternative.\n",
        "\n",
        "We recreate these analyses here. In each case, we will set the $\\alpha = 0.05$ level (a typical Type I error probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S88t4LtzTyK"
      },
      "outputs": [],
      "source": [
        "dissox = bbnwr[\"Dissolved Oxygen (mg/L)\"].dropna()\n",
        "dissox_sem = dissox.std() / np.sqrt(len(dissox))\n",
        "\n",
        "dissox_z = (dissox.mean() - 7) / dissox_sem\n",
        "dissox_z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt6u0nYXzTyL"
      },
      "source": [
        "Under the null hypothesis the statistic above should fall with within 2 standard deviations of 0 with 95% probability. As we observe a statistic outside of this range, we reject the null hypothesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGupbNrEzTyL"
      },
      "outputs": [],
      "source": [
        "salinity = bbnwr[\"Salinity (ppt)\"].dropna()\n",
        "salinity_sem = salinity.std() / np.sqrt(len(salinity))\n",
        "salinity_z = (salinity.mean() - 0.8)/ salinity_sem\n",
        "salinity_z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrW_NVHzTyL"
      },
      "source": [
        "Likewise for salinity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prhosbg2zTyM"
      },
      "source": [
        "Why whould we use $\\alpha = 0.05$? This is a common tolerance for Type I error, but certainly not the only value.\n",
        "\n",
        "Instead, we could ask, \"What is is the smallest rejection region that would include the observed test statistic?\" Since we are thinking about two tailed tests, this is equivalent to asking, \"What is the probabiliy of observing a Gaussian random variable exceeding $|\\bar X - c| / \\text{SEM}$ or less than $-|\\bar X - c| / \\text{SEM})$?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLEY4Y-FzTyM"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "dissox_z_abs = np.abs(dissox_z)\n",
        "norm.cdf(- dissox_z_abs) + (1 - norm.cdf(dissox_z_abs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWuOvrkHzTyM"
      },
      "outputs": [],
      "source": [
        "## or by the symmetry of the Gaussian distribution\n",
        "2 * norm.cdf(- dissox_z_abs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAv61UXAzTyM"
      },
      "source": [
        "For any $\\alpha$ a person selects, we can decide if we reject the null hypothesis if the $p$-value is less than $\\alpha$. With a $p$-value of $3.7 \\times 10^{-8}$, we would reject this null hypothesis with almost any typical significance/$\\alpha$ level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrERAecszTyM"
      },
      "source": [
        "Repeat this computation for the salinity test. Would we reject this hypothesis at the $\\alpha = 0.001$ level?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFfvrqfCzTyN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "\n",
        "salinity_z_abs = np.abs(salinity_z)\n",
        "\n",
        "norm.cdf(- salinity_z_abs) + (1 - norm.cdf(salinity_z_abs))\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "k5K6jDigRyr9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOG-c4A5SSnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "2 * norm.cdf(- dissox_z_abs)\n",
        "</details>"
      ],
      "metadata": {
        "id": "K8eWr-M_STSn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir4wR1mY4uUw"
      },
      "source": [
        "## Other Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNPYl_kzTyN"
      },
      "source": [
        "Many other statistics have approximate Gaussian distributions in large samples with the following standard errors:\n",
        "\n",
        "* Difference of means ($\\bar X - \\bar Y$) with sample sizes $n$ and $m$: $\\sqrt{S_x^2/n + S_y^2/m}$\n",
        "* Proportions ($\\hat p$): $\\sqrt{\\hat p (1 - \\hat p) / n}$\n",
        "* Correlation: $1/\\sqrt{n}$\n",
        "\n",
        "\n",
        "\n",
        "Let's test the hypothesis that in the population of all tests, 1/3 of them come from the Bay against the two sided alternative that the proportion is some other value.\n",
        "\n",
        "Create a new column that indicates if a reading came from the \"Bay\" or a numbered site.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBd8cv3VzTyN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COSnIDNgXWGp"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "bbnwr[\"is_bay\"] = bbnwr[\"Site_Id\"] == \"Bay\"\n",
        "p_hat = bbnwr[\"is_bay\"].mean()\n",
        "p_hat\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIvnkbL1cu4"
      },
      "source": [
        "Calculate the estimated standard error of this statistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko1NP5VU1x-T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohlRn9KUXPk9"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "se_p_hat = np.sqrt(p_hat * (1 - p_hat) / bbnwr[\"is_bay\"].count())\n",
        "\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxdsdvWC5Yrs"
      },
      "source": [
        "Using the sample proportion and the estimated standard error for that proportion, apply the concept of a Z-test to this hypothesis test. At the 5% level, would you reject the hypothesis that 1/3 of the observations would come from the Bay in the population of all observations?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x3LPXW95vhq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htrxhw6JXJhp"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "(p_hat - 1/3) / se_p_hat\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAkNXzAB1Jcc"
      },
      "source": [
        "At the 5% level, would we reject the hypothesis that 1/3 of the population measurements occur at the Bay?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o4MAiH7Pdp5"
      },
      "source": [
        "# **Hypothesis Testing Between Hour_Minute and Temperature**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IS2RIzK6LC-"
      },
      "source": [
        "Let's do one last test to practice this technique. The following creates a new column that gives time in fractional hours (e.g., 9:30 becomes 9.5).\n",
        "\n",
        "Create a table with just the non-missing entries for `hour_minute` and `AirTemp (C)`. Create a scatter plot of these two columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc4ZXgx_7Csi"
      },
      "outputs": [],
      "source": [
        "bbnwr[\"dt\"] = pd.to_datetime(bbnwr[\"Read_Date\"] + \" \" + bbnwr[\"Time (24:00)\"], errors = 'coerce')\n",
        "bbnwr[\"hour_minute\"] = bbnwr[\"dt\"].dt.hour + bbnwr[\"dt\"].dt.minute/60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eliqO4rQ7rah"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbbqFe_iXDXx"
      },
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "time_temp = bbnwr[[\"hour_minute\", \"AirTemp (C)\"]].dropna()\n",
        "sb.scatterplot(data = time_temp, x = \"hour_minute\", y = \"AirTemp (C)\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFeif6gQcmy"
      },
      "source": [
        "Let's test the hypothesis that the correlation between the \"hour_minute\" and air temperature is zero or less against the alternative that is is more than zero.\n",
        "\n",
        "Compute the standard error for the correlation coefficient statistic for this setting and perform a Z-test selecting a rejection region that will ensure a 0.16 probability of rejecting a true null hypothesis. What do you conclude?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL2piK29Rwjj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ZasQMMRiSP"
      },
      "source": [
        "<details>\n",
        "\n",
        "For this test, after subtracting the hypothesized value of 0 and dividing by the SE, the result should be approximately distributed as N(0,1). So to control Type I error at the 0.16 level, we would reject for a test statistic greater than 1.\n",
        "\n",
        "```\n",
        "(n, _) = time_temp.shape\n",
        "\n",
        "corr_se = 1/np.sqrt(n)\n",
        "\n",
        "test_stat = (time_temp.corr()[\"hour_minute\"][\"AirTemp (C)\"] - 0) / corr_se\n",
        "test_stat\n",
        "```\n",
        "\n",
        "With a value exceeding 1, we would reject this null hypothesis\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7a388a7"
      },
      "source": [
        "\n",
        "## Extra Section: Quick and Easy Regression Analysis with `statsmodels`\n",
        "Let's try something a little more advanced! We'll use `statsmodels` to fit a linear regression model\n",
        "and see how it handles the heavy lifting for us. Here, we aim to analyze how one of our variables - Water Temp (C) relates to `Dissolved Oxygen (mg/L)`.\n",
        "\n",
        "Ready for some stats magic?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f733fecc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "bbnwr_cleaned = bbnwr[['Dissolved Oxygen (mg/L)', 'Water Temp (?C)']].dropna()\n",
        "X = bbnwr_cleaned['Water Temp (?C)']  # Independent variable\n",
        "y = bbnwr_cleaned['Dissolved Oxygen (mg/L)']  # Dependent variable\n",
        "\n",
        "# Adding a constant term for intercept in the model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Display the model summary\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acdb5c28"
      },
      "source": [
        "\n",
        "### Interpreting the Results\n",
        "Check out the table above! `statsmodels` has provided us with an abundance of information:\n",
        "- **Coefficient**: This tells us the relationship between Temperature and Dissolved Oxygen levels. It's -0.1566 here, so the relation is negative. Hence as water temperature increases, the dissolved oxygen decreases, which corroborates with the known scientific fact that **warmer water holds less dissolved oxygen.**\n",
        "- **P-value**: Is the relationship statistically significant? P-value here is 0.000, hence it is significant.\n",
        "- **R-squared**: How well does our model explain the variability in Dissolved Oxygen levels? R-sq here is 0.263, so doesn't explain much but only to some extent.\n",
        "\n",
        "Using `statsmodels` is like having a personal statistician; it automates calculations, so you donâ€™t have to sweat the details."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}